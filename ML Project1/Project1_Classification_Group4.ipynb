{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMdZ_Z7gw81o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Categorical Variables\n",
    "Preprocess Categorical Data function takes in the dataframe and changes the categorical columns to mapping 0, 1 values or one hot encoding vectors.\n",
    "\n",
    "Mapping to 0 or 1: Maps the categorical column if only two values are possible for that attribute\n",
    "\n",
    "One Hot Encoding: One Hot encoding for the categorical column if that attribute has more than two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljBb-6MW0VxQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_categorical_data(X):\n",
    "    type_list = X.dtypes\n",
    "    cols = X.columns\n",
    "    for col in cols:\n",
    "        if X[col].dtype == np.object:\n",
    "            temp = set(X[col])\n",
    "            if len(temp) == 2:\n",
    "                temp_dict = dict()\n",
    "                for key in temp:\n",
    "                    if sum(X[col] == key) > (len(X[col]) / 2):\n",
    "                        temp_dict[key] = 1\n",
    "                    else:\n",
    "                        temp_dict[key] = 0\n",
    "                X.loc[:,col] = X[col].map(temp_dict).astype(int)\n",
    "            else:\n",
    "                np.unique(X[col])\n",
    "                emb = pd.get_dummies(X[col], columns=col, prefix=col)\n",
    "                X = pd.concat([X, emb], axis=1)\n",
    "                X.drop([col], axis=1, inplace=True)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n",
    "This function imputes the missing rows data based on the imputed_column given \n",
    "as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pb9PvRK80WEK"
   },
   "outputs": [],
   "source": [
    "def impute_missing_data(X, imputed_column):\n",
    "    cols = X.columns\n",
    "    for col in cols:\n",
    "        if X[col].dtype == np.float:\n",
    "          X[col] = X.groupby(imputed_column)[col].transform(lambda grp: grp.fillna(grp.mean()))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Don't get Kicked\n",
    "Data Source: https://www.kaggle.com/c/DontGetKicked\n",
    "\n",
    "\n",
    "Data Description: The data is of cars sold at various auctions and the parameters. And the task is to predict whether the car bought is a bad buy or not.\n",
    "\n",
    "We artificially induced missing data by replacing values with NaN using python code. Also i sampled the data for 50 models of cars to make the dataset smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "vnzRiwjYw9jt",
    "outputId": "d6644357-efb1-4eda-dec6-e086bae101b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IsBadBuy', 'PurchDate', 'Auction', 'VehYear', 'VehicleAge', 'Make',\n",
       "       'Model', 'Trim', 'SubModel', 'Color', 'Transmission', 'VehOdo',\n",
       "       'Nationality', 'Size', 'TopThreeAmericanName',\n",
       "       'MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
       "       'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n",
       "       'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
       "       'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', 'BYRNO',\n",
       "       'VehBCost', 'IsOnlineSale', 'WarrantyCost'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('carvana.csv', mode='r') as csv_file:\n",
    "    df = pd.read_csv(csv_file)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping off unnecessary columns\n",
    "\n",
    "We are dropping off the columns which are IDs of the buyer and auction ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJNyoCqWxwt8"
   },
   "outputs": [],
   "source": [
    "df_new = df.drop(columns=['BYRNO','Make','SubModel','Trim','Color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the missing data\n",
    "\n",
    "We are imputing the missing data in real value columns with the average value of the same model cars\n",
    "\n",
    "We also removed the Model column since the variable IsBadBuy depends on the cost at which car was bought and the estimated price of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xzk_YeDXxzEM"
   },
   "outputs": [],
   "source": [
    "df_new = impute_missing_data(df_new, 'Model')\n",
    "df_new = df_new.drop(columns=['Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing categorical data\n",
    "\n",
    "Mapping or one hot encoding the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Jphzk-6zP6m"
   },
   "outputs": [],
   "source": [
    "df_new = preprocess_categorical_data(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Null rows\n",
    "\n",
    "The imputing method works if the missing values belongs to a model which has multiple columns to infer values from.  But if the model has only one row we don't have any reference values to impute from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boK32GSA2R8b"
   },
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assignments\n",
    "\n",
    "Assigning the predictor variables to X \n",
    "Assigning the output variable to Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqveSC0l7OsJ"
   },
   "outputs": [],
   "source": [
    "X = df_new.drop(columns=['IsBadBuy'])\n",
    "y = df_new['IsBadBuy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and Scaling\n",
    "\n",
    "We are splitting the data to training and testing data.\n",
    "\n",
    "Once the splitting is done we are scaling  the training data columns using the normal scaler.\n",
    "\n",
    "And the testing data is modified using the scaling parameters of the training data.\n",
    "\n",
    "We are using the Normal Scaler since normal scaler preserves the statistical properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dnorkhx37aJp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=0)\n",
    "cols = X_train.columns\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train,columns = cols)\n",
    "X_test = pd.DataFrame(X_test,columns = cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "We used grid search along with 10 Fold Cross Validation on number of neighbors to choose the best parameter based on accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxVtCGNz7zoD"
   },
   "outputs": [],
   "source": [
    "#KNN Classification\n",
    "knn = KNeighborsClassifier()\n",
    "k_range = list(range(1, 31))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9JJIjFMA-iv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                         13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                         23, 24, 25, 26, 27, 28, 29, 30]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix of the best K Nearest Neighbor Model.\n",
    "\n",
    "As you can see because of the imbalanced data set we can see even though most of the positive examples are misclassified we are getting high accuracy.\n",
    "\n",
    "One way to deal with imbalanced datasets is to use weighted models with balanced accuracy as parameter.\n",
    "\n",
    "Or we could use a sampling strategy to deal with data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "921K7pJxG6KA",
    "outputId": "2da79589-cffe-4f0d-e14a-bd9f58f8f9bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[968,   1],\n",
       "       [131,   3]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid.best_params_\n",
    "knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n",
    "knn.fit(X_train, y_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy of the best K Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "De4U_hlNHTp5",
    "outputId": "86fb4536-068e-407e-a92a-4d2846b59445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8803263825929284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier Model\n",
    "\n",
    "We are using lbfgs solver (Limitied Memory BFGS Model) which is a quasi-newton method for faster convergence. \n",
    "\n",
    "Since, LBFGS works only on l2 penalty we are performing grid search on l2 penalty with various c values and 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3K_e9GaIMRZ"
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression(multi_class=\"auto\",solver=\"lbfgs\", max_iter=10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed4OS8ZtIQYg"
   },
   "outputs": [],
   "source": [
    "param_grid = {  'penalty' : ['l2'],\n",
    "     'C' : np.logspace(-4, 4, 20)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mC9t2MfYL7Tn"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "EEtY0EFYMT4C",
    "outputId": "ece96b0e-b951-4b84-bc15-e81937975447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=10000, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=0, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.0000000...,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that some positive examples are correctly being classified.\n",
    "\n",
    "Which also proves that with more data we will have a better model with high sensitivity and specificity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "gsaMd810Mn4B",
    "outputId": "78820dfd-ba56-49d2-b46b-46edc433938f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[893,  76],\n",
       "       [115,  19]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C = param_grid['C'][np.argmin(grid.cv_results_['rank_test_score'])]\n",
    "#y_test_pred = grid.decision_function(X_test)\n",
    "#y_test_pred\n",
    "best_params = grid.best_params_\n",
    "logistic_reg = LogisticRegression(multi_class=\"auto\",solver=\"liblinear\", max_iter=10000, random_state=0, penalty=best_params['penalty'], C= best_params['C'])\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "y_test_pred = logistic_reg.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy of the Best logistic classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a2kEnFLIM9qM",
    "outputId": "235cdb2a-526f-4b9c-f433-abc88e44d001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268359020852222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC Model\n",
    "\n",
    "We are using the Linear SVC model with squared_hinge loss with both l1 and l2 penalty.\n",
    "\n",
    "l1 Model leads to much sparser model compared to l2 model.\n",
    "\n",
    "Grid Search is performed on various C Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTd5xBDxPLbX"
   },
   "outputs": [],
   "source": [
    "#Linear SVM\n",
    "param_grid = {'penalty':['l1','l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQuo5W7ce0ew"
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(max_iter = 1e+4, random_state=0, loss='squared_hinge', dual=False)\n",
    "grid = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this training because liblinear model is unable to fit the model with the given data in mentioned iterations limit.\n",
    "\n",
    "Tried with various max_iterations to see if the model converges. But the model didn't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JRLFMlA3frOI",
    "outputId": "39f34e1a-7b3f-4a23-987f-420b43351098"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\sweth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=10000.0,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=0, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy of the best Linear SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "V2elBkKkgBPv",
    "outputId": "7089a3bb-5f2c-4984-a1f2-b18ed034af6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8785131459655485"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid.best_params_\n",
    "svc = LinearSVC(penalty = best_params['penalty'],C=best_params['C'], max_iter = 1e+4, random_state=0, loss='squared_hinge', dual=False)\n",
    "svc.fit(X_train, y_train)\n",
    "y_test_pred = svc.predict(X_test)\n",
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix of the best Linear SVM Model. As you can see, since no positive samples are being classified correctly this linear svm model is not a great model despite the same accuracy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[969,   0],\n",
       "       [134,   0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel SVM\n",
    "\n",
    "We are using grid search to train for different values of gamma and c parameters to find best rbf kernel svm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhAM5GNiCwgJ"
   },
   "outputs": [],
   "source": [
    "#RBF Kernel SVM\n",
    "tuned_parameters = {'gamma': [1e-3,1e-2,1e-1,1],\n",
    "                     'C': [1, 10, 100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYHpp7R23LYr"
   },
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 0, kernel = 'rbf')\n",
    "grid = GridSearchCV(svm, tuned_parameters, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "-3_6UhxB3VBM",
    "outputId": "0eb725f3-0a36-4bfa-ab44-a106c4e408eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=0, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [1, 10, 100, 1000],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MojM-D_B3x2w"
   },
   "outputs": [],
   "source": [
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the best rbf kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gnPU01g_OlA2",
    "outputId": "90034a54-94cc-47b7-a6da-90f2bd498bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8776065276518585"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(random_state = 0, kernel = 'rbf', C =best_params['C'], gamma=best_params['gamma'] )\n",
    "svm.fit(X_train, y_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the confusion matrix, we can say that the rbf kernel svm is also not a good model due to low sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[967,   2],\n",
       "       [133,   1]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Polynomial Kernel SVM\n",
    "\n",
    "We are using gridsearch with 5 fold cross validation with various degrees and C values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2KQGHCWOn_T"
   },
   "outputs": [],
   "source": [
    "#Polynomial Kernel SVM\n",
    "tuned_parameters = {'degree': [1,2,3,4,5,6,7],\n",
    "                     'C': [1, 10, 100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-sEAdDfOxxS"
   },
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 0, kernel = 'poly', gamma = 'auto')\n",
    "grid = GridSearchCV(svm, tuned_parameters, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "Zn-InSE7O3MT",
    "outputId": "aa72e899-c11e-454d-9ecf-2353d590e3cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto', kernel='poly', max_iter=-1,\n",
       "                           probability=False, random_state=0, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [1, 10, 100, 1000],\n",
       "                         'degree': [1, 2, 3, 4, 5, 6, 7]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8kohSi2mO4C_",
    "outputId": "ea21ce4b-fe1f-4d99-e185-df0a43575d97"
   },
   "outputs": [],
   "source": [
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy of the best polyomial kernel svm model is 87.03%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gxRhrDsOpRI8",
    "outputId": "e05c6197-e58c-4ae2-f604-b125a13aabaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8703535811423391"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(random_state=0, degree=best_params['degree'], C=best_params['C'], gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From confusion matrix, we can see that the sensitivty of this model is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[959,  10],\n",
       "       [133,   1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "We use grid-search on various min_samples_split values with 10 fold cross-validation to find the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKkJ_Ahkpke1"
   },
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "sample_split_range = list(range(2, 50))\n",
    "param_grid = dict(min_samples_split=sample_split_range)\n",
    "grid = GridSearchCV(dtc, param_grid, cv=10, scoring='balanced_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "qH6q-iVf6DKS",
    "outputId": "456924c2-21d0-4175-9650-6f7ae05edf3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=0,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                               12, 13, 14, 15, 16, 17, 18, 19,\n",
       "                                               20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                                               28, 29, 30, 31, ...]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='balanced_accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlbHoWHh6SJl"
   },
   "outputs": [],
   "source": [
    "best_param = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy of the decision tree classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E487dBMb6tDM",
    "outputId": "e5472f6a-515e-4f01-bc34-8e1179d19128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8485947416137806"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=0, min_samples_split=best_param['min_samples_split'])\n",
    "dtc.fit(X_train, y_train)\n",
    "y_test_pred = dtc.predict(X_test)\n",
    "accuracy = sum(y_test == y_test_pred)/len(y_test_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the sensitivty value is low but better than the SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb0VD9e97C-F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[918,  51],\n",
       "       [116,  18]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing all the test accuracies and the sensitivity ratio of different models. We can say that the Decision Tree Classification Model is the best model inspite of SVM Models with higher test accuracies.\n",
    "\n",
    "Since, the Decision tree classification model is the best model that gave high test accuracy and high sensitivity values.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Project-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
